{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import chromadb\n",
    "from chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sanchez/VScode_consolidated/task1/db/'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIR = os.path.dirname(os.path.abspath(__name__))\n",
    "DB_PATH = os.path.join(DIR, 'db/')\n",
    "DB_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the running ChromaDB instance\n",
    "client = chromadb.PersistentClient(\n",
    "  path=DB_PATH,\n",
    "  settings=Settings(),\n",
    "  tenant=DEFAULT_TENANT,\n",
    "  database=DEFAULT_DATABASE,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"pdf_name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or get a collection\n",
    "collection = client.create_collection(collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add data to the collection\n",
    "documents = [\"This is a beach\", \"The forest is green\", \"Urban areas are crowded\"]\n",
    "metadatas = [{\"category\": \"beach\"}, {\"category\": \"forest\"}, {\"category\": \"urban\"}]\n",
    "ids = [\"1\", \"2\", \"3\"]\n",
    "\n",
    "collection.add(documents=documents, metadatas=metadatas, ids=ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['2', '3', '4']], 'distances': [[0.8691518582313926, 0.9016820837823974, 0.9688559958091476]], 'metadatas': [[None, None, None]], 'embeddings': None, 'documents': [['the default setting of 16 to 64MB is usually too small. If you get error s that\\nclasses are not found, check your CLASSPATH : does it include weka.jar ? You\\ncan explicitly set CLASSPATH via the-cpcommand line option as well.\\nWe will begin by describing basic concepts and ideas. Then, we will desc ribe\\ntheweka.filters package, which is used to transform input data, e.g. for\\npreprocessing, transformation, feature generation and so on.', 'Then we will focus on the machine learning algorithms themselves. The se\\nare called Classiﬁers in WEKA. We will restrict ourselves to common set tings\\nfor all classiﬁers and shortly note representatives for all main app roaches in\\nmachine learning.\\nAfterwards, practical examples are given.\\nFinally, in the docdirectory of WEKA you ﬁnd a documentation of all java\\nclasses within WEKA. Prepare to use it since this overview is not intend ed to', 'be complete. If you want to know exactly what is going on, take a look at the\\nmostly well-documented source code, which can be found in weka-src.jar and\\ncan be extracted via the jarutility from the Java Development Kit (or any\\narchive program that can handle ZIP ﬁles).\\n13']], 'uris': None, 'data': None, 'included': ['metadatas', 'documents', 'distances']}\n"
     ]
    }
   ],
   "source": [
    "# Query the collection\n",
    "results = collection.query(\n",
    "    query_texts=[\"weka supporting dataset formats\"],\n",
    "    n_results=3  # Number of results to return\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanchez/VScode_consolidated/task1/venv/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DOG_embeddings = model.encode([\"DOG\"])\n",
    "dog_embeddings = model.encode([\"dog\"])\n",
    "(DOG_embeddings == dog_embeddings).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False False False\n"
     ]
    }
   ],
   "source": [
    "good_embeddings = model.encode([\"good\"])\n",
    "better_embeddings = model.encode([\"better\"])\n",
    "best_embeddings = model.encode([\"best\"])\n",
    "print(\n",
    "  (good_embeddings == better_embeddings).all(),\n",
    "  (good_embeddings == best_embeddings).all(),\n",
    "  (better_embeddings == best_embeddings).all()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "very_good_embeddings = model.encode([\"very good\"])\n",
    "better_embeddings_embeddings = model.encode([\"better\"])\n",
    "print((very_good_embeddings == better_embeddings_embeddings).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"pdf_name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma(\n",
    "    collection_name=collection_name,\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"db/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add documents\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "document_1 = Document(\n",
    "    page_content=\"I had chocolate chip pancakes and scrambled eggs for breakfast this morning.\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=1,\n",
    ")\n",
    "\n",
    "document_2 = Document(\n",
    "    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    "    id=2,\n",
    ")\n",
    "\n",
    "document_3 = Document(\n",
    "    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=3,\n",
    ")\n",
    "\n",
    "document_4 = Document(\n",
    "    page_content=\"Robbers broke into the city bank and stole $1 million in cash.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    "    id=4,\n",
    ")\n",
    "\n",
    "document_5 = Document(\n",
    "    page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=5,\n",
    ")\n",
    "\n",
    "document_6 = Document(\n",
    "    page_content=\"Is the new iPhone worth the price? Read this review to find out.\",\n",
    "    metadata={\"source\": \"website\"},\n",
    "    id=6,\n",
    ")\n",
    "\n",
    "document_7 = Document(\n",
    "    page_content=\"The top 10 soccer players in the world right now.\",\n",
    "    metadata={\"source\": \"website\"},\n",
    "    id=7,\n",
    ")\n",
    "\n",
    "document_8 = Document(\n",
    "    page_content=\"LangGraph is the best framework for building stateful, agentic applications!\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=8,\n",
    ")\n",
    "\n",
    "document_9 = Document(\n",
    "    page_content=\"The stock market is down 500 points today due to fears of a recession.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    "    id=9,\n",
    ")\n",
    "\n",
    "document_10 = Document(\n",
    "    page_content=\"I have a bad feeling I am going to get deleted :(\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=10,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_11 = Document(\n",
    "    page_content=\"I had chocolate chip pancakes and scrambled eggs for breakfast this morning.\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "document_12 = Document(\n",
    "    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    ")\n",
    "\n",
    "document_13 = Document(\n",
    "    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0638bc76-8c5d-48e4-8c75-c3fcaf38478f',\n",
       " '33fa3b82-e01c-4313-8707-b97cb2db4044',\n",
       " '3c48a076-f043-4e4e-b663-7a1406080471']"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [\n",
    "    document_11,\n",
    "    document_12,\n",
    "    document_13,\n",
    "]\n",
    "\n",
    "# uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "ids = [11, 12, 13]\n",
    "vector_store.add_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### upload to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "DIR = os.path.dirname(os.path.abspath(__name__))\n",
    "DB_PATH = os.path.join(DIR, 'db/')\n",
    "\n",
    "from chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings\n",
    "import chromadb\n",
    "persistent_client = chromadb.PersistentClient(\n",
    "    path=DB_PATH,\n",
    "    settings=Settings(),\n",
    "    tenant=DEFAULT_TENANT,\n",
    "    database=DEFAULT_DATABASE,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanchez/VScode_consolidated/task1/venv/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "collection_name = \"weka\"\n",
    "vector_store = Chroma(\n",
    "  client=persistent_client,\n",
    "  collection_name=collection_name,\n",
    "  embedding_function=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "  chunk_size=500,\n",
    "  chunk_overlap=50,\n",
    "  length_function=len,\n",
    "  keep_separator=False,\n",
    "  add_start_index=True,\n",
    "  is_separator_regex=False,\n",
    "  separators=['\\n\\n', '\\n', ' ', '']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the large document into small documnets\n",
    "async def split_document(Document:Document):\n",
    "  return Document.metadata, splitter.create_documents([Document.page_content])\n",
    "  # return splitter.split_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "# Split text into chunks\n",
    "async def split_text(input:str|List[str]):\n",
    "  if isinstance(input,str):\n",
    "    return splitter.create_documents([input])\n",
    "  if isinstance(input,List[str]):\n",
    "    return splitter.create_documents(input)\n",
    "  return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pdf file\n",
    "async def load_pdf(pdf_file):\n",
    "  # Initialize the PDF loader inside the function\n",
    "  loader = PyPDFLoader(\n",
    "    pdf_file,\n",
    "    extract_images=True,\n",
    "    headers=None,\n",
    "    extraction_mode=\"plain\",\n",
    "  )\n",
    "  pacge_count = 1\n",
    "  async for page in loader.alazy_load():\n",
    "    document = Document(\n",
    "      metadata=page.metadata,\n",
    "      page_content=page.page_content\n",
    "    )\n",
    "    print(\"page:\",pacge_count)\n",
    "    pacge_count += 1\n",
    "\n",
    "    yield await split_document(document) # Yield each chunk one at a time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def upload_pdf(pdf_file:str):\n",
    "  ids = list()\n",
    "  id_calibration = 1\n",
    "  async for metadata, chunks in load_pdf(pdf_file):\n",
    "    for id, chunk in enumerate(chunks):\n",
    "      chunk.metadata.update(metadata)\n",
    "      ids.append(id_calibration+id)\n",
    "\n",
    "    results = vector_store.add_documents(documents=chunks)\n",
    "    print(len(results))\n",
    "    # break\n",
    "    id_calibration = ids[-1]+1\n",
    "    ids = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page: 1\n",
      "4\n",
      "page: 2\n",
      "5\n",
      "page: 3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "  await upload_pdf(\"WekaManual_13to15.pdf\")\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'page': 2, 'source': 'WekaManual_13to15.pdf', 'start_index': 889}, page_content='the main() routine of weka.core.Instances :\\njava weka.core.Instances data/soybean.arff\\nweka.core oﬀers some other useful routines, e.g. converters.C45Loader and\\nconverters.CSVLoader ,whichcanbeusedtoimportC45datasetsandcomma/tab-\\nseparated datasets respectively, e.g.:\\njava weka.core.converters.CSVLoader data.csv > data.arf f\\njava weka.core.converters.C45Loader c45_filestem > data .arff')]\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search(\n",
    "  \"what are weka supported dataset formats?\", k=1)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrate LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmware.models import ModelCatalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load the model and make a basic inference\n",
    "model = ModelCatalog().load_model(\"bling-phi-3-gguf\", temperature=0.0, sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llm_response': 'weka supports the ARFF format.',\n",
       " 'usage': {'input': 160,\n",
       "  'output': 8,\n",
       "  'total': 168,\n",
       "  'metric': 'tokens',\n",
       "  'processing_time': 60.769243240356445}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.inference(\"what dataset file format does weka support\", add_context='the main() routine of weka.core.Instances :\\njava weka.core.Instances data/soybean.arff\\nweka.core oﬀers some other useful routines, e.g. converters.C45Loader and\\nconverters.CSVLoader ,whichcanbeusedtoimportC45datasetsandcomma/tab-\\nseparated datasets respectively, e.g.:\\njava weka.core.converters.CSVLoader data.csv > data.arf f\\njava weka.core.converters.C45Loader c45_filestem > data .arff')\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qwen2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-generation\", model=\"Qwen/Qwen2.5-0.5B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation history\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "# Prepare input for the pipeline\n",
    "input_text = messages[-1][\"content\"]  # Get the latest user message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response: Who are you? I am an AI language model created by Alibaba Cloud, and my primary purpose is to assist users in generating human-like text. My ability to understand natural language and generate coherent responses allows me to be used for a variety of applications such\n"
     ]
    }
   ],
   "source": [
    "# Generate a response\n",
    "response = pipe(input_text, max_length=50, num_return_sequences=1)\n",
    "\n",
    "# Print the generated response\n",
    "print(\"Generated Response:\", response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline to connect database and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tile set up vectorstore\n",
    "import os\n",
    "DIR = os.path.dirname(os.path.abspath(__name__))\n",
    "DB_PATH = os.path.join(DIR, 'db/')\n",
    "\n",
    "from chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings\n",
    "import chromadb\n",
    "persistent_client = chromadb.PersistentClient(\n",
    "  path=DB_PATH,\n",
    "  settings=Settings(),\n",
    "  tenant=DEFAULT_TENANT,\n",
    "  database=DEFAULT_DATABASE,)\n",
    "    \n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "collection_name = \"weka\"\n",
    "vectorstore = Chroma(\n",
    "  client=persistent_client,\n",
    "  collection_name=collection_name,\n",
    "  embedding_function=embeddings,\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanchez/VScode_consolidated/task1/venv/lib/python3.12/site-packages/langsmith/client.py:354: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "prompt_template = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Answer the question based only on the following context:\\n\\n{context}\\n\\nQuestion: {question}\\n'), additional_kwargs={})])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(template)\n",
    "# prompt = \"Give me a short introduction to large language model.\"\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='Answer the question based only on the following context:\\n\\nthe main() routine of weka.core.Instances :\\njava weka.core.Instances data/soybean.arff\\nweka.core oﬀers some other useful routines, e.g. converters.C45Loader and\\nconverters.CSVLoader ,whichcanbeusedtoimportC45datasetsandcomma/tab-\\nseparated datasets respectively, e.g.:\\njava weka.core.converters.CSVLoader data.csv > data.arf f\\njava weka.core.converters.C45Loader c45_filestem > data .arff\\n\\n14 CHAPTER 1. A COMMAND-LINE PRIMER\\n1.2 Basic concepts\\n1.2.1 Dataset\\nA set of data items, the dataset, is a very basic concept of machine learning. A\\ndatasetisroughlyequivalenttoatwo-dimensionalspreadsheetor databasetable.\\nIn WEKA, it is implemented by the weka.core.Instances class. A dataset is\\na collection of examples, each one of class weka.core.Instance . Each Instance\\nconsists of a number of attributes, any of which can be nominal (= one of a\\n\\nversions, date/time attribute types are also supported.\\nBy default, the last attribute is considered the class/target varia ble, i.e. the\\nattribute which should be predicted as a function of all other attrib utes. If this\\nis not the case, specify the target variable via -c. The attribute numbers are\\none-based indices, i.e. -c 1speciﬁes the ﬁrst attribute.\\nSome basic statistics and validation of given ARFF ﬁles can be obtained via\\nthe main() routine of weka.core.Instances :\\n\\nthe default setting of 16 to 64MB is usually too small. If you get error s that\\nclasses are not found, check your CLASSPATH : does it include weka.jar ? You\\ncan explicitly set CLASSPATH via the-cpcommand line option as well.\\nWe will begin by describing basic concepts and ideas. Then, we will desc ribe\\ntheweka.filters package, which is used to transform input data, e.g. for\\npreprocessing, transformation, feature generation and so on.\\n\\nQuestion: what dataset file format does weka support\\n', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt_template)\n",
    "chain.invoke(\"what dataset file format does weka support\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Answer the question based only on the following context:\\n\\nthe main() routine of weka.core.Instances :\\njava weka.core.Instances data/soybean.arff\\nweka.core oﬀers some other useful routines, e.g. converters.C45Loader and\\nconverters.CSVLoader ,whichcanbeusedtoimportC45datasetsandcomma/tab-\\nseparated datasets respectively, e.g.:\\njava weka.core.converters.CSVLoader data.csv > data.arf f\\njava weka.core.converters.C45Loader c45_filestem > data .arff\\n\\n14 CHAPTER 1. A COMMAND-LINE PRIMER\\n1.2 Basic concepts\\n1.2.1 Dataset\\nA set of data items, the dataset, is a very basic concept of machine learning. A\\ndatasetisroughlyequivalenttoatwo-dimensionalspreadsheetor databasetable.\\nIn WEKA, it is implemented by the weka.core.Instances class. A dataset is\\na collection of examples, each one of class weka.core.Instance . Each Instance\\nconsists of a number of attributes, any of which can be nominal (= one of a\\n\\nversions, date/time attribute types are also supported.\\nBy default, the last attribute is considered the class/target varia ble, i.e. the\\nattribute which should be predicted as a function of all other attrib utes. If this\\nis not the case, specify the target variable via -c. The attribute numbers are\\none-based indices, i.e. -c 1speciﬁes the ﬁrst attribute.\\nSome basic statistics and validation of given ARFF ﬁles can be obtained via\\nthe main() routine of weka.core.Instances :\\n\\nthe default setting of 16 to 64MB is usually too small. If you get error s that\\nclasses are not found, check your CLASSPATH : does it include weka.jar ? You\\ncan explicitly set CLASSPATH via the-cpcommand line option as well.\\nWe will begin by describing basic concepts and ideas. Then, we will desc ribe\\ntheweka.filters package, which is used to transform input data, e.g. for\\npreprocessing, transformation, feature generation and so on.\\n\\nQuestion: what dataset file format does weka support\\n'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are a chatbot. You are here to help users with their questions.'},\n",
       " {'role': 'user',\n",
       "  'content': \"Answer the question based only on the following context:\\n\\nthe main() routine of weka.core.Instances :\\njava weka.core.Instances data/soybean.arff\\nweka.core oﬀers some other useful routines, e.g. converters.C45Loader and\\nconverters.CSVLoader ,whichcanbeusedtoimportC45datasetsandcomma/tab-\\nseparated datasets respectively, e.g.:\\njava weka.core.converters.CSVLoader data.csv > data.arf f\\njava weka.core.converters.C45Loader c45_filestem > data .arff\\n\\n14 CHAPTER 1. A COMMAND-LINE PRIMER\\n1.2 Basic concepts\\n1.2.1 Dataset\\nA set of data items, the dataset, is a very basic concept of machine learning. A\\ndatasetisroughlyequivalenttoatwo-dimensionalspreadsheetor databasetable.\\nIn WEKA, it is implemented by the weka.core.Instances class. A dataset is\\na collection of examples, each one of class weka.core.Instance . Each Instance\\nconsists of a number of attributes, any of which can be nominal (= one of a\\n\\nversions, date/time attribute types are also supported.\\nBy default, the last attribute is considered the class/target varia ble, i.e. the\\nattribute which should be predicted as a function of all other attrib utes. If this\\nis not the case, specify the target variable via -c. The attribute numbers are\\none-based indices, i.e. -c 1speciﬁes the ﬁrst attribute.\\nSome basic statistics and validation of given ARFF ﬁles can be obtained via\\nthe main() routine of weka.core.Instances :\\n\\nthe default setting of 16 to 64MB is usually too small. If you get error s that\\nclasses are not found, check your CLASSPATH : does it include weka.jar ? You\\ncan explicitly set CLASSPATH via the-cpcommand line option as well.\\nWe will begin by describing basic concepts and ideas. Then, we will desc ribe\\ntheweka.filters package, which is used to transform input data, e.g. for\\npreprocessing, transformation, feature generation and so on.\\n\\nQuestion: what dataset file format does weka support\\n'\"}]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a chatbot. You are here to help users with their questions.\"},\n",
    "    {\"role\": \"user\", \"content\": query} #propmt\n",
    "]\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|im_start|>system\\nYou are a chatbot. You are here to help users with their questions.<|im_end|>\\n<|im_start|>user\\nAnswer the question based only on the following context:\\n\\nthe main() routine of weka.core.Instances :\\njava weka.core.Instances data/soybean.arff\\nweka.core oﬀers some other useful routines, e.g. converters.C45Loader and\\nconverters.CSVLoader ,whichcanbeusedtoimportC45datasetsandcomma/tab-\\nseparated datasets respectively, e.g.:\\njava weka.core.converters.CSVLoader data.csv > data.arf f\\njava weka.core.converters.C45Loader c45_filestem > data .arff\\n\\n14 CHAPTER 1. A COMMAND-LINE PRIMER\\n1.2 Basic concepts\\n1.2.1 Dataset\\nA set of data items, the dataset, is a very basic concept of machine learning. A\\ndatasetisroughlyequivalenttoatwo-dimensionalspreadsheetor databasetable.\\nIn WEKA, it is implemented by the weka.core.Instances class. A dataset is\\na collection of examples, each one of class weka.core.Instance . Each Instance\\nconsists of a number of attributes, any of which can be nominal (= one of a\\n\\nversions, date/time attribute types are also supported.\\nBy default, the last attribute is considered the class/target varia ble, i.e. the\\nattribute which should be predicted as a function of all other attrib utes. If this\\nis not the case, specify the target variable via -c. The attribute numbers are\\none-based indices, i.e. -c 1speciﬁes the ﬁrst attribute.\\nSome basic statistics and validation of given ARFF ﬁles can be obtained via\\nthe main() routine of weka.core.Instances :\\n\\nthe default setting of 16 to 64MB is usually too small. If you get error s that\\nclasses are not found, check your CLASSPATH : does it include weka.jar ? You\\ncan explicitly set CLASSPATH via the-cpcommand line option as well.\\nWe will begin by describing basic concepts and ideas. Then, we will desc ribe\\ntheweka.filters package, which is used to transform input data, e.g. for\\npreprocessing, transformation, feature generation and so on.\\n\\nQuestion: what dataset file format does weka support\\n'<|im_end|>\\n<|im_start|>assistant\\n\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[151644,   8948,    198,   2610,    525,    264,   6236,   6331,     13,\n",
       "           1446,    525,   1588,    311,   1492,   3847,    448,    862,   4755,\n",
       "             13, 151645,    198, 151644,    872,    198,  16141,    279,   3405,\n",
       "           3118,   1172,    389,    279,   2701,   2266,   1447,   1782,   1887,\n",
       "            368,  14021,    315,    582,   4554,   4871,   5337,   9436,   6260,\n",
       "          10042,    582,   4554,   4871,   5337,   9436,    821,     14,    704,\n",
       "             88,  17479,  16711,    542,    198,    896,   4554,   4871,    297,\n",
       "         145730,    388,   1045,   1008,   5390,  29497,     11,    384,   1302,\n",
       "             13,  88888,    727,     19,     20,   9181,    323,    198,  14166,\n",
       "            388,    727,  17803,   9181,   1154,   8206,   4814,   1371,   2591,\n",
       "            983,    474,     34,     19,     20,  65546,    437,  45386,  78859,\n",
       "           6913,    325,  49600,  29425,  15576,     11,    384,   1302,     13,\n",
       "            510,  10042,    582,   4554,   4871,  24012,    388,    727,  17803,\n",
       "           9181,    821,  11219,    861,    821,  16711,     69,    282,    198,\n",
       "          10042,    582,   4554,   4871,  24012,    388,    727,     19,     20,\n",
       "           9181,    272,     19,     20,   2458,  64088,    861,    821,    659,\n",
       "            277,    542,    271,     16,     19,  95324,    220,     16,     13,\n",
       "            362,  40218,   8125,   3981,   8575,  56221,    198,     16,     13,\n",
       "             17,  14625,  18940,    198,     16,     13,     17,     13,     16,\n",
       "          39183,    198,     32,    738,    315,    821,   3589,     11,    279,\n",
       "          10337,     11,    374,    264,   1602,   6770,   7286,    315,   5662,\n",
       "           6832,     13,    362,    198,  21378,    285,   1432,    398,  25310,\n",
       "          11769,    983,    266,   1126,   1737,  18161,   1127,  20717,  15119,\n",
       "            269,  71240,   5028,    480,    624,    641,  19677,  26444,     11,\n",
       "            432,    374,  11537,    553,    279,    582,   4554,   4871,   5337,\n",
       "           9436,    536,     13,    362,  10337,    374,    198,     64,   4426,\n",
       "            315,  10295,     11,   1817,    825,    315,    536,    582,   4554,\n",
       "           4871,  12688,    659,   8886,  19283,    198,   6254,   1671,    315,\n",
       "            264,   1372,    315,   8201,     11,    894,    315,    892,    646,\n",
       "            387,  46755,  38738,    825,    315,    264,    271,  28290,     11,\n",
       "           2400,  35263,   7035,   4494,    525,   1083,   7248,    624,   1359,\n",
       "           1638,     11,    279,   1537,   7035,    374,   6509,    279,    536,\n",
       "          88868,    762,    685,  12422,     11,    600,   1734,     13,    279,\n",
       "            198,   9116,    892,   1265,    387,  19149,    438,    264,    729,\n",
       "            315,    678,   1008,  17329,    220,   2095,     13,   1416,    419,\n",
       "            198,    285,    537,    279,   1142,     11,  13837,    279,   2169,\n",
       "           3890,   4566,    481,     66,     13,    576,   7035,   5109,    525,\n",
       "            198,    603,   5980,  14937,     11,    600,   1734,     13,    481,\n",
       "             66,    220,     16,   9535,     72, 144300,    288,    279,  32495,\n",
       "          70731,  58025,   7035,    624,   8373,   6770,  13142,    323,  10519,\n",
       "            315,   2661,   6261,   1748,  32495,  70731,    642,    646,    387,\n",
       "          12180,   4566,    198,   1782,   1887,    368,  14021,    315,    582,\n",
       "           4554,   4871,   5337,   9436,  14512,   1782,   1638,   6243,    315,\n",
       "            220,     16,     21,    311,    220,     21,     19,   8412,    374,\n",
       "           5990,   2238,   2613,     13,   1416,    498,    633,   1465,    274,\n",
       "            429,    198,   8855,    525,    537,   1730,     11,   1779,    697,\n",
       "          27939,  13593,    549,   1558,    432,   2924,    582,   4554,  27628,\n",
       "            937,   1446,    198,   4814,  20975,    738,  27939,  13593,   4566,\n",
       "            279,   1786,     79,   5631,   1555,   2999,    438,   1632,    624,\n",
       "           1654,    686,   3161,    553,  22692,   6770,  18940,    323,   6708,\n",
       "             13,   5005,     11,    582,    686,   6560,  20131,     68,    198,\n",
       "          15765,  52313,  41707,   6328,     11,    892,    374,   1483,    311,\n",
       "           5165,   1946,    821,     11,    384,   1302,     13,    369,    198,\n",
       "           1726,  20660,     11,  17991,     11,   4565,   9471,    323,    773,\n",
       "            389,    382,  14582,     25,   1128,  10337,   1034,   3561,   1558,\n",
       "            582,   4554,   1824,    198,      6, 151645,    198, 151644,  77091,\n",
       "            198]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1]])}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens = tokenizer([template], return_tensors=\"pt\").to(model.device)\n",
    "input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[151644,   8948,    198,   2610,    525,    264,   6236,   6331,     13,\n",
       "           1446,    525,   1588,    311,   1492,   3847,    448,    862,   4755,\n",
       "             13, 151645,    198, 151644,    872,    198,  16141,    279,   3405,\n",
       "           3118,   1172,    389,    279,   2701,   2266,   1447,   1782,   1887,\n",
       "            368,  14021,    315,    582,   4554,   4871,   5337,   9436,   6260,\n",
       "          10042,    582,   4554,   4871,   5337,   9436,    821,     14,    704,\n",
       "             88,  17479,  16711,    542,    198,    896,   4554,   4871,    297,\n",
       "         145730,    388,   1045,   1008,   5390,  29497,     11,    384,   1302,\n",
       "             13,  88888,    727,     19,     20,   9181,    323,    198,  14166,\n",
       "            388,    727,  17803,   9181,   1154,   8206,   4814,   1371,   2591,\n",
       "            983,    474,     34,     19,     20,  65546,    437,  45386,  78859,\n",
       "           6913,    325,  49600,  29425,  15576,     11,    384,   1302,     13,\n",
       "            510,  10042,    582,   4554,   4871,  24012,    388,    727,  17803,\n",
       "           9181,    821,  11219,    861,    821,  16711,     69,    282,    198,\n",
       "          10042,    582,   4554,   4871,  24012,    388,    727,     19,     20,\n",
       "           9181,    272,     19,     20,   2458,  64088,    861,    821,    659,\n",
       "            277,    542,    271,     16,     19,  95324,    220,     16,     13,\n",
       "            362,  40218,   8125,   3981,   8575,  56221,    198,     16,     13,\n",
       "             17,  14625,  18940,    198,     16,     13,     17,     13,     16,\n",
       "          39183,    198,     32,    738,    315,    821,   3589,     11,    279,\n",
       "          10337,     11,    374,    264,   1602,   6770,   7286,    315,   5662,\n",
       "           6832,     13,    362,    198,  21378,    285,   1432,    398,  25310,\n",
       "          11769,    983,    266,   1126,   1737,  18161,   1127,  20717,  15119,\n",
       "            269,  71240,   5028,    480,    624,    641,  19677,  26444,     11,\n",
       "            432,    374,  11537,    553,    279,    582,   4554,   4871,   5337,\n",
       "           9436,    536,     13,    362,  10337,    374,    198,     64,   4426,\n",
       "            315,  10295,     11,   1817,    825,    315,    536,    582,   4554,\n",
       "           4871,  12688,    659,   8886,  19283,    198,   6254,   1671,    315,\n",
       "            264,   1372,    315,   8201,     11,    894,    315,    892,    646,\n",
       "            387,  46755,  38738,    825,    315,    264,    271,  28290,     11,\n",
       "           2400,  35263,   7035,   4494,    525,   1083,   7248,    624,   1359,\n",
       "           1638,     11,    279,   1537,   7035,    374,   6509,    279,    536,\n",
       "          88868,    762,    685,  12422,     11,    600,   1734,     13,    279,\n",
       "            198,   9116,    892,   1265,    387,  19149,    438,    264,    729,\n",
       "            315,    678,   1008,  17329,    220,   2095,     13,   1416,    419,\n",
       "            198,    285,    537,    279,   1142,     11,  13837,    279,   2169,\n",
       "           3890,   4566,    481,     66,     13,    576,   7035,   5109,    525,\n",
       "            198,    603,   5980,  14937,     11,    600,   1734,     13,    481,\n",
       "             66,    220,     16,   9535,     72, 144300,    288,    279,  32495,\n",
       "          70731,  58025,   7035,    624,   8373,   6770,  13142,    323,  10519,\n",
       "            315,   2661,   6261,   1748,  32495,  70731,    642,    646,    387,\n",
       "          12180,   4566,    198,   1782,   1887,    368,  14021,    315,    582,\n",
       "           4554,   4871,   5337,   9436,  14512,   1782,   1638,   6243,    315,\n",
       "            220,     16,     21,    311,    220,     21,     19,   8412,    374,\n",
       "           5990,   2238,   2613,     13,   1416,    498,    633,   1465,    274,\n",
       "            429,    198,   8855,    525,    537,   1730,     11,   1779,    697,\n",
       "          27939,  13593,    549,   1558,    432,   2924,    582,   4554,  27628,\n",
       "            937,   1446,    198,   4814,  20975,    738,  27939,  13593,   4566,\n",
       "            279,   1786,     79,   5631,   1555,   2999,    438,   1632,    624,\n",
       "           1654,    686,   3161,    553,  22692,   6770,  18940,    323,   6708,\n",
       "             13,   5005,     11,    582,    686,   6560,  20131,     68,    198,\n",
       "          15765,  52313,  41707,   6328,     11,    892,    374,   1483,    311,\n",
       "           5165,   1946,    821,     11,    384,   1302,     13,    369,    198,\n",
       "           1726,  20660,     11,  17991,     11,   4565,   9471,    323,    773,\n",
       "            389,    382,  14582,     25,   1128,  10337,   1034,   3561,   1558,\n",
       "            582,   4554,   1824,    198,      6, 151645,    198, 151644,  77091,\n",
       "            198,     54,  52313,  11554,   3807,  19856,    369,  10337,   3542,\n",
       "             13,   3776,   4185,   3561,    374,    279,  27445,    320,   1092,\n",
       "           1728,   6222,  11584,    657,  24979,      8,   3561,     13,  13293,\n",
       "           5411,   3561,    374,    279,   1644,    542,    320,   3907,     12,\n",
       "           1130,  15042,    568,   1205,   4554,   1083,   5707,  88888,    311,\n",
       "           2795,   5257,   4494,    315,  29425,   1741,    438,    356,     19,\n",
       "             13,     20,  29425,    323,  31683,  72692,   2750,  29465,  18104,\n",
       "              8,   3542,     13, 151645]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tokens = model.generate(\n",
    "    **input_tokens,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([    54,  52313,  11554,   3807,  19856,    369,  10337,   3542,     13,\n",
       "           3776,   4185,   3561,    374,    279,  27445,    320,   1092,   1728,\n",
       "           6222,  11584,    657,  24979,      8,   3561,     13,  13293,   5411,\n",
       "           3561,    374,    279,   1644,    542,    320,   3907,     12,   1130,\n",
       "          15042,    568,   1205,   4554,   1083,   5707,  88888,    311,   2795,\n",
       "           5257,   4494,    315,  29425,   1741,    438,    356,     19,     13,\n",
       "             20,  29425,    323,  31683,  72692,   2750,  29465,  18104,      8,\n",
       "           3542,     13, 151645])]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tokens = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(input_tokens.input_ids, output_tokens)\n",
    "]\n",
    "output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Weka supports several formats for dataset files. One common format is the CSV (Comma-Separated Values) format. Another popular format is the Arff (Attribute-Value Format). Weka also provides converters to load various types of datasets such as C4.5 datasets and comma-separated values (.csv) files.'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = tokenizer.batch_decode(output_tokens, skip_special_tokens=True)[0]\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title dsfds\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tile set up vectorstore\n",
    "import os\n",
    "DIR = os.path.dirname(os.path.abspath(__name__))\n",
    "DB_PATH = os.path.join(DIR, 'db/')\n",
    "\n",
    "from chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings\n",
    "import chromadb\n",
    "persistent_client = chromadb.PersistentClient(\n",
    "  path=DB_PATH,\n",
    "  settings=Settings(),\n",
    "  tenant=DEFAULT_TENANT,\n",
    "  database=DEFAULT_DATABASE,)\n",
    "    \n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "collection_name = \"weka\"\n",
    "vectorstore = Chroma(\n",
    "  client=persistent_client,\n",
    "  collection_name=collection_name,\n",
    "  embedding_function=embeddings,\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "def get_prompt(prompt_value):\n",
    "  return prompt_value.messages[0].content\n",
    "\n",
    "def set_role(prompt):\n",
    "  return [\n",
    "    {\"role\": \"system\", \"content\": \"You are a chatbot. You are here to help users with their questions.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "  ]\n",
    "\n",
    "def get_chat_template(messages):\n",
    "  return tokenizer.apply_chat_template(\n",
    "      messages,\n",
    "      tokenize=False,\n",
    "      add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "def input_tokenizer(template):\n",
    "  return tokenizer([template], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "def generate_ids(input_tokens):\n",
    "  generated_ids = model.generate(\n",
    "      **input_tokens,\n",
    "      max_new_tokens=512\n",
    "    )\n",
    "  return input_tokens, generated_ids\n",
    "\n",
    "def get_output_ids(inputs):\n",
    "  input_tokens,generated_ids = inputs\n",
    "  return [output_ids[len(input_ids):] for input_ids, output_ids in zip(input_tokens.input_ids, generated_ids)]\n",
    "\n",
    "def output_tokenizer(output_tokens):\n",
    "  return tokenizer.batch_decode(output_tokens, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt_template  # Invoke the prompt based on query and document context\n",
    "    | get_prompt  # Get the prompt from the generated value\n",
    "    | set_role  # Set the role for the conversation (system, user, etc.)\n",
    "    | get_chat_template  # Apply chat template formatting to the messages\n",
    "    | input_tokenizer  # Tokenize the chat template into input tokens\n",
    "    | generate_ids  # Generate ids and pass the input tokens along\n",
    "    | get_output_ids  # Process input tokens and generated ids to get output tokens\n",
    "    | output_tokenizer  # Tokenize the output tokens into the final response\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Weka framework supports several dataset formats including CSV files (.csv), ARFF (Attribute-Relation Format) files, and OAR (Open Attribute Relation) files.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\"what dataset file format does weka support\")\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
