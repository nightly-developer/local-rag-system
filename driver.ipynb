{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import chromadb\n",
    "from chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sanchez/VScode_consolidated/task1/db/'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIR = os.path.dirname(os.path.abspath(__name__))\n",
    "DB_PATH = os.path.join(DIR, 'db/')\n",
    "DB_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the running ChromaDB instance\n",
    "client = chromadb.PersistentClient(\n",
    "  path=DB_PATH,\n",
    "  settings=Settings(),\n",
    "  tenant=DEFAULT_TENANT,\n",
    "  database=DEFAULT_DATABASE,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or get a collection\n",
    "collection = client.create_collection(\"weka\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add data to the collection\n",
    "documents = [\"This is a beach\", \"The forest is green\", \"Urban areas are crowded\"]\n",
    "metadatas = [{\"category\": \"beach\"}, {\"category\": \"forest\"}, {\"category\": \"urban\"}]\n",
    "ids = [\"1\", \"2\", \"3\"]\n",
    "\n",
    "collection.add(documents=documents, metadatas=metadatas, ids=ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['2', '3', '4']], 'distances': [[0.8691518582313926, 0.9016820837823974, 0.9688559958091476]], 'metadatas': [[None, None, None]], 'embeddings': None, 'documents': [['the default setting of 16 to 64MB is usually too small. If you get error s that\\nclasses are not found, check your CLASSPATH : does it include weka.jar ? You\\ncan explicitly set CLASSPATH via the-cpcommand line option as well.\\nWe will begin by describing basic concepts and ideas. Then, we will desc ribe\\ntheweka.filters package, which is used to transform input data, e.g. for\\npreprocessing, transformation, feature generation and so on.', 'Then we will focus on the machine learning algorithms themselves. The se\\nare called Classiﬁers in WEKA. We will restrict ourselves to common set tings\\nfor all classiﬁers and shortly note representatives for all main app roaches in\\nmachine learning.\\nAfterwards, practical examples are given.\\nFinally, in the docdirectory of WEKA you ﬁnd a documentation of all java\\nclasses within WEKA. Prepare to use it since this overview is not intend ed to', 'be complete. If you want to know exactly what is going on, take a look at the\\nmostly well-documented source code, which can be found in weka-src.jar and\\ncan be extracted via the jarutility from the Java Development Kit (or any\\narchive program that can handle ZIP ﬁles).\\n13']], 'uris': None, 'data': None, 'included': ['metadatas', 'documents', 'distances']}\n"
     ]
    }
   ],
   "source": [
    "# Query the collection\n",
    "results = collection.query(\n",
    "    query_texts=[\"weka supporting dataset formats\"],\n",
    "    n_results=3  # Number of results to return\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanchez/VScode_consolidated/task1/venv/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.True_"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DOG_embeddings = model.encode([\"DOG\"])\n",
    "dog_embeddings = model.encode([\"dog\"])\n",
    "(DOG_embeddings == dog_embeddings).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False False False\n"
     ]
    }
   ],
   "source": [
    "good_embeddings = model.encode([\"good\"])\n",
    "better_embeddings = model.encode([\"better\"])\n",
    "best_embeddings = model.encode([\"best\"])\n",
    "print(\n",
    "  (good_embeddings == better_embeddings).all(),\n",
    "  (good_embeddings == best_embeddings).all(),\n",
    "  (better_embeddings == best_embeddings).all()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "very_good_embeddings = model.encode([\"very good\"])\n",
    "better_embeddings_embeddings = model.encode([\"better\"])\n",
    "print((very_good_embeddings == better_embeddings_embeddings).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma(\n",
    "    collection_name=\"documnet_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"db/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add documents\n",
    "from uuid import uuid4\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "document_1 = Document(\n",
    "    page_content=\"I had chocolate chip pancakes and scrambled eggs for breakfast this morning.\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=1,\n",
    ")\n",
    "\n",
    "document_2 = Document(\n",
    "    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    "    id=2,\n",
    ")\n",
    "\n",
    "document_3 = Document(\n",
    "    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=3,\n",
    ")\n",
    "\n",
    "document_4 = Document(\n",
    "    page_content=\"Robbers broke into the city bank and stole $1 million in cash.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    "    id=4,\n",
    ")\n",
    "\n",
    "document_5 = Document(\n",
    "    page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=5,\n",
    ")\n",
    "\n",
    "document_6 = Document(\n",
    "    page_content=\"Is the new iPhone worth the price? Read this review to find out.\",\n",
    "    metadata={\"source\": \"website\"},\n",
    "    id=6,\n",
    ")\n",
    "\n",
    "document_7 = Document(\n",
    "    page_content=\"The top 10 soccer players in the world right now.\",\n",
    "    metadata={\"source\": \"website\"},\n",
    "    id=7,\n",
    ")\n",
    "\n",
    "document_8 = Document(\n",
    "    page_content=\"LangGraph is the best framework for building stateful, agentic applications!\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=8,\n",
    ")\n",
    "\n",
    "document_9 = Document(\n",
    "    page_content=\"The stock market is down 500 points today due to fears of a recession.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    "    id=9,\n",
    ")\n",
    "\n",
    "document_10 = Document(\n",
    "    page_content=\"I have a bad feeling I am going to get deleted :(\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    "    id=10,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_11 = Document(\n",
    "    page_content=\"I had chocolate chip pancakes and scrambled eggs for breakfast this morning.\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "document_12 = Document(\n",
    "    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    ")\n",
    "\n",
    "document_13 = Document(\n",
    "    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0638bc76-8c5d-48e4-8c75-c3fcaf38478f',\n",
       " '33fa3b82-e01c-4313-8707-b97cb2db4044',\n",
       " '3c48a076-f043-4e4e-b663-7a1406080471']"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [\n",
    "    document_11,\n",
    "    document_12,\n",
    "    document_13,\n",
    "]\n",
    "\n",
    "# uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "ids = [11, 12, 13]\n",
    "vector_store.add_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### upload to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "DIR = os.path.dirname(os.path.abspath(__name__))\n",
    "DB_PATH = os.path.join(DIR, 'db/')\n",
    "\n",
    "from chromadb.config import DEFAULT_TENANT, DEFAULT_DATABASE, Settings\n",
    "import chromadb\n",
    "persistent_client = chromadb.PersistentClient(\n",
    "    path=DB_PATH,\n",
    "    settings=Settings(),\n",
    "    tenant=DEFAULT_TENANT,\n",
    "    database=DEFAULT_DATABASE,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanchez/VScode_consolidated/task1/venv/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "collection_name = \"weka\"\n",
    "vector_store = Chroma(\n",
    "  client=persistent_client,\n",
    "  collection_name=collection_name,\n",
    "  embedding_function=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "  chunk_size=500,\n",
    "  chunk_overlap=50,\n",
    "  length_function=len,\n",
    "  keep_separator=False,\n",
    "  add_start_index=True,\n",
    "  is_separator_regex=False,\n",
    "  separators=['\\n\\n', '\\n', ' ', '']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the large document into small documnets\n",
    "async def split_document(Document:Document):\n",
    "  return Document.metadata, splitter.create_documents([Document.page_content])\n",
    "  # return splitter.split_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "# Split text into chunks\n",
    "async def split_text(input:str|List[str]):\n",
    "  if isinstance(input,str):\n",
    "    return splitter.create_documents([input])\n",
    "  if isinstance(input,List[str]):\n",
    "    return splitter.create_documents(input)\n",
    "  return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pdf file\n",
    "async def load_pdf(pdf_file):\n",
    "  # Initialize the PDF loader inside the function\n",
    "  loader = PyPDFLoader(\n",
    "    pdf_file,\n",
    "    extract_images=True,\n",
    "    headers=None,\n",
    "    extraction_mode=\"plain\",\n",
    "  )\n",
    "  pacge_count = 1\n",
    "  async for page in loader.alazy_load():\n",
    "    document = Document(\n",
    "      metadata=page.metadata,\n",
    "      page_content=page.page_content\n",
    "    )\n",
    "    print(\"page:\",pacge_count)\n",
    "    pacge_count += 1\n",
    "\n",
    "    yield await split_document(document) # Yield each chunk one at a time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def upload_pdf(pdf_file:str):\n",
    "  ids = list()\n",
    "  id_calibration = 1\n",
    "  async for metadata, chunks in load_pdf(pdf_file):\n",
    "    for id, chunk in enumerate(chunks):\n",
    "      chunk.metadata.update(metadata)\n",
    "      ids.append(id_calibration+id)\n",
    "\n",
    "    results = vector_store.add_documents(documents=chunks)\n",
    "    print(len(results))\n",
    "    # break\n",
    "    id_calibration = ids[-1]+1\n",
    "    ids = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page: 1\n",
      "4\n",
      "page: 2\n",
      "5\n",
      "page: 3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "  await upload_pdf(\"WekaManual_13to15.pdf\")\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'page': 2, 'source': 'WekaManual_13to15.pdf', 'start_index': 889}, page_content='the main() routine of weka.core.Instances :\\njava weka.core.Instances data/soybean.arff\\nweka.core oﬀers some other useful routines, e.g. converters.C45Loader and\\nconverters.CSVLoader ,whichcanbeusedtoimportC45datasetsandcomma/tab-\\nseparated datasets respectively, e.g.:\\njava weka.core.converters.CSVLoader data.csv > data.arf f\\njava weka.core.converters.C45Loader c45_filestem > data .arff')]\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search(\n",
    "  \"what are weka supported dataset formats?\", k=1)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrate LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmware.models import ModelCatalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[37mWARNING: ModelCatalog - load_model - fetching model - bling-phi-3-gguf - from remote repository using pull_snapshot_from_hf - this may take a couple of minutes the first time.\u001b[39m\n",
      "/home/sanchez/VScode_consolidated/task1/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/sanchez/VScode_consolidated/task1/venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1204: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n",
      "Fetching 7 files: 100%|██████████| 7/7 [00:40<00:00,  5.72s/it]\n"
     ]
    }
   ],
   "source": [
    "# to load the model and make a basic inference\n",
    "model = ModelCatalog().load_model(\"bling-phi-3-gguf\", temperature=0.0, sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llm_response': 'weka supports the ARFF format.',\n",
       " 'usage': {'input': 160,\n",
       "  'output': 8,\n",
       "  'total': 168,\n",
       "  'metric': 'tokens',\n",
       "  'processing_time': 58.7152144908905}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.inference(\"what dataset file format does weka support\", add_context='the main() routine of weka.core.Instances :\\njava weka.core.Instances data/soybean.arff\\nweka.core oﬀers some other useful routines, e.g. converters.C45Loader and\\nconverters.CSVLoader ,whichcanbeusedtoimportC45datasetsandcomma/tab-\\nseparated datasets respectively, e.g.:\\njava weka.core.converters.CSVLoader data.csv > data.arf f\\njava weka.core.converters.C45Loader c45_filestem > data .arff')\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
